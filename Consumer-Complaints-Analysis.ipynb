{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center face=\"verdana\" style=\"color:#6699FF\";font-family:verdana>Consumer Complaints Exploration</h1>\n",
    "<div style=\"width:200px; background-color:white; height:120px;\">\n",
    "\t\t<div style=\"width: 950px; height: 90px;\">\n",
    "\t\t\t<img src=\"https://media.amazonwebservices.com/blog/2007/big_pbaws_logo_300px.jpg\" width=\"150\" height=\"61\" alt=\"aws\" />\n",
    "\t\t\t<img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" width=\"94\" height=\"50\" alt=\"spark\" />\n",
    "\t\t</div>\n",
    "\t</div>\n",
    "<h3 align=center>Meera Lakhavani</h3>\n",
    "<h4 align=center>August 2016</h4>\n",
    "\n",
    "\n",
    "\n",
    "**Goals:**\n",
    "- Apply data science workflow to dataset using PySpark\n",
    "- Generate insights by merging with other datasets and/or using NLP (such as TF-IDF)\n",
    "- Understand and practice AWS setup and how to monitor/allocate computing resources properly\n",
    "\n",
    "**Data:**\n",
    "<br>Obtained from: http://catalog.data.gov/dataset<br>\n",
    "From <u>Consumer Financial Protection Bureau</u> — \"These are complaints we’ve received about financial products and services\"\n",
    "\n",
    "**Business Question to Answer:** <br>\n",
    "<FONT \n",
    "style=\"BACKGROUND-COLOR: #A9F5F2\">\n",
    "What factors drive or predict financial complaints? <br>\n",
    "What are consumers complaining about?</FONT> (Subsegment by product and company)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> Contents:<u>\n",
    "\n",
    "<OL>\n",
    "<LI>Configure Spark Environment\n",
    "<LI>Load Data\n",
    "<LI>Explore Data\n",
    "\n",
    "<LI>Clean Data\n",
    "<LI>Visualize Data\n",
    "<LI>Build Models\n",
    "    <UL>\n",
    "    <LI>NLP: Word2Vec and TF-IDF (term frequency inverse document frequency)\n",
    "    <LI>Regression?\n",
    "    </UL>\n",
    "<LI>Model Validation\n",
    "    <LI>?\n",
    "    </UL> \n",
    "</OL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#6699FF\">Section 0 - Configure Spark Environment</h2>\n",
    "\n",
    "- Import libraries &#10004;\n",
    "- Understand how much data to process, and estimate computing resource required\n",
    "- Set Spark configs to meet desired computing resource requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#6699FF\">0.1 - Import Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Spark related libraries\n",
    "from pyspark import SparkContext,SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.functions import col, date_format, udf\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Python formats and others\n",
    "import sys\n",
    "import time\n",
    "import boto3\n",
    "from boto3.s3.transfer import S3Transfer\n",
    "import datetime\n",
    "\n",
    "# Data manipulation\n",
    "% matplotlib inline\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import vincent\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.feature import IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#6699FF\">0.2 - Estimate Computing Resource</h3>\n",
    "**TBC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#6699FF\">0.3 - Create and Set Spark Context</h3>\n",
    "- Stop and create a new Spark context (Advanced - Optional)\n",
    "- Show Current Spark Configurations\n",
    "- Configure Spark Context\n",
    "**TBC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f81aa8c6bd0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.dynamicAllocation.enabled:  PySparkShell\n",
      "spark.kryoserializer.buffer.max.mb:  None\n",
      "spark.submit.deployMode:          client\n",
      "spark.executor.memory:            5120M\n",
      "spark.serializer:                 None\n",
      "spark.executor.cores:             4\n",
      "spark.master:                     yarn\n",
      "spark.serializer:                 None\n",
      "spark.dynamicAllocation.enabled:  true\n"
     ]
    }
   ],
   "source": [
    "# Here are a list of key parameters of Spark Context you can configure\n",
    "print 'spark.dynamicAllocation.enabled: ', sc._conf.get('spark.app.name')\n",
    "print 'spark.kryoserializer.buffer.max.mb: ', sc._conf.get('spark.kryoserializer.buffer.max.mb')\n",
    "print 'spark.submit.deployMode:         ', sc._conf.get('spark.submit.deployMode')\n",
    "print 'spark.executor.memory:           ', sc._conf.get('spark.executor.memory')\n",
    "print 'spark.serializer:                ', sc._conf.get('spark.serializer')\n",
    "print 'spark.executor.cores:            ', sc._conf.get('spark.executor.cores')\n",
    "print 'spark.master:                    ', sc._conf.get('spark.master')\n",
    "print 'spark.serializer:                ', sc._conf.get('spark.serializer')\n",
    "print 'spark.dynamicAllocation.enabled: ', sc._conf.get('spark.dynamicAllocation.enabled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#6699FF\">Section 1 - Data Access & Storage</h2>\n",
    "\n",
    "- Load data in csv format from AWS S3\n",
    "- Datasets schema validation (Advanced)\n",
    "- Datasets consolidation\n",
    "- Save consolidated data in appropriate storage structure and format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#6699FF\">1.1 - Load Data in CSV Format From AWS S3</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:#6699FF\">Create HiveContext from SparkContext</h4>\n",
    "In PySpark, sqlContext/hiveContext IS created automatically "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7f81b80d4b10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new sqlcontext (example)\n",
    "from pyspark.sql import HiveContext\n",
    "#hiveCtx = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:#6699FF\">Load and save data using HiveContext and Databricks CSV Package</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input file directory, one single file\n",
    "file_dir = 's3://consumer-complaint-data/Consumer_Complaints.csv'\n",
    "\n",
    "\n",
    "# load one csv data file\n",
    "df = (sqlContext.read\n",
    "            .format('com.databricks.spark.csv')\n",
    "            .options(header='true',inferSchema=\"true\",delimiter=\",\") # csv library will infer schema automatically\n",
    "            .load(file_dir))  # specify the file directory here\n",
    "            #.cache()) # cache is optional, if you want to save data in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:#6699FF\">Explore Columns (schema)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date received: string (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Sub-product: string (nullable = true)\n",
      " |-- Issue: string (nullable = true)\n",
      " |-- Sub-issue: string (nullable = true)\n",
      " |-- Consumer complaint narrative: string (nullable = true)\n",
      " |-- Company public response: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- ZIP code: string (nullable = true)\n",
      " |-- Tags: string (nullable = true)\n",
      " |-- Consumer consent provided?: string (nullable = true)\n",
      " |-- Submitted via: string (nullable = true)\n",
      " |-- Date sent to company: string (nullable = true)\n",
      " |-- Company response to consumer: string (nullable = true)\n",
      " |-- Timely response?: string (nullable = true)\n",
      " |-- Consumer disputed?: string (nullable = true)\n",
      " |-- Complaint ID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+\n",
      "|Date received|             Product|               Issue|\n",
      "+-------------+--------------------+--------------------+\n",
      "|   07/29/2013|       Consumer Loan|Managing the loan...|\n",
      "|   07/29/2013|Bank account or s...|Using a debit or ...|\n",
      "+-------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out the first 2 rows of the selected columns\n",
    "df.select(\"Date received\",\"Product\",\"Issue\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#6699FF\">1.2 - Schema and Data Quality Validation</h3>\n",
    "**TBC**\n",
    "Complete playbook steps\n",
    "<br>Note: my current data is all in one file (until I merge with other CapIQ or financial statement data) so I do not need a consistency check or consolidation at this time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#6699FF\">Section 2 - Exploratory Data Analysis</h2>\n",
    "- Descriptive statistics for data &#10004;\n",
    "- Treatment of raw data (null values, outliers, etc)\n",
    "- Basic EDA summary statistics and plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:#6699FF\">2.1 Descriptive Statistics Summary</h3>\n",
    "All columns are strings, so exploration of their distribution is the option as opposed to numerical statistics<br>\n",
    "Conversion of strings to datetime to see time distribution is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|summary|\n",
      "+-------+\n",
      "|  count|\n",
      "|   mean|\n",
      "| stddev|\n",
      "|    min|\n",
      "|    max|\n",
      "+-------+\n",
      "\n",
      "Runtime:  0.120000001043 s\n"
     ]
    }
   ],
   "source": [
    "start = os.times()[-1]\n",
    "\n",
    "# calculate summary statistics\n",
    "df_desp_num = df.describe()\n",
    "\n",
    "# print out the result\n",
    "gap = 5\n",
    "for i in range(0,len(df_desp_num.columns),gap):\n",
    "    df_desp_num.select(df_desp_num.columns[i:(i+gap)]).show()\n",
    "    \n",
    "end = os.times()[-1]\n",
    "print \"Runtime: \", end - start , \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             Company|count|\n",
      "+--------------------+-----+\n",
      "|     Bank of America|56529|\n",
      "|Wells Fargo & Com...|42597|\n",
      "|                null|37747|\n",
      "|             Equifax|34491|\n",
      "|JPMorgan Chase & Co.|34471|\n",
      "|            Experian|33137|\n",
      "|TransUnion Interm...|27571|\n",
      "|            Citibank|26793|\n",
      "|               Ocwen|21005|\n",
      "|         Capital One|15937|\n",
      "| Nationstar Mortgage|13315|\n",
      "| Synchrony Financial| 9751|\n",
      "|        U.S. Bancorp| 9694|\n",
      "|Ditech Financial LLC| 8950|\n",
      "|Navient Solutions...| 8096|\n",
      "|       PNC Bank N.A.| 7121|\n",
      "|Encore Capital Group| 6372|\n",
      "|HSBC North Americ...| 6252|\n",
      "|                Amex| 5911|\n",
      "|SunTrust Banks, Inc.| 4951|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Runtime:  6.76000000164 s\n"
     ]
    }
   ],
   "source": [
    "start = os.times()[-1]\n",
    "\n",
    "col_name = \"Company\"\n",
    "\n",
    "# Calculate histogram of string column\n",
    "col_dist_company = ( df.groupby(col_name)                 # aggregate data by column value\n",
    "                    .count()                           # count number of rows for each column value\n",
    "                    .sort(\"count\", ascending=False) )  # sort column value in the descending order\n",
    "\n",
    "# print out distribution\n",
    "col_dist_company.show()\n",
    "\n",
    "end = os.times()[-1]\n",
    "print \"Runtime: \", end - start , \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|             Company|State|count|\n",
      "+--------------------+-----+-----+\n",
      "|                null| null|37747|\n",
      "|     Bank of America|   CA|10851|\n",
      "|Wells Fargo & Com...|   CA| 8167|\n",
      "|     Bank of America|   FL| 6434|\n",
      "|JPMorgan Chase & Co.|   CA| 6092|\n",
      "|            Experian|   CA| 4790|\n",
      "|Wells Fargo & Com...|   FL| 4587|\n",
      "|            Experian|   TX| 4222|\n",
      "|             Equifax|   CA| 4187|\n",
      "|             Equifax|   TX| 3916|\n",
      "|            Citibank|   CA| 3878|\n",
      "|               Ocwen|   CA| 3823|\n",
      "|             Equifax|   FL| 3719|\n",
      "|JPMorgan Chase & Co.|   NY| 3717|\n",
      "|TransUnion Interm...|   CA| 3442|\n",
      "|TransUnion Interm...|   TX| 3423|\n",
      "|     Bank of America|   NY| 3288|\n",
      "|JPMorgan Chase & Co.|   FL| 3207|\n",
      "|            Experian|   FL| 3204|\n",
      "|            Citibank|   NY| 3046|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Runtime:  5.16000000015 s\n"
     ]
    }
   ],
   "source": [
    "start = os.times()[-1]\n",
    "\n",
    "col_name = \"Company\"\n",
    "col_name_2 = \"State\"\n",
    "\n",
    "# Calculate histogram of string column\n",
    "col_dist_company_state = ( df.groupby(col_name, col_name_2)                 # aggregate data by column value\n",
    "                    .count()                           # count number of rows for each column value\n",
    "                    .sort(\"count\", ascending=False) )  # sort column value in the descending order\n",
    "\n",
    "# print out distribution\n",
    "col_dist_company_state.show()\n",
    "\n",
    "end = os.times()[-1]\n",
    "print \"Runtime: \", end - start , \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|             Product| count|\n",
      "+--------------------+------+\n",
      "|            Mortgage|193283|\n",
      "|     Debt collection|107248|\n",
      "|    Credit reporting|101259|\n",
      "|         Credit card| 69961|\n",
      "|Bank account or s...| 66711|\n",
      "|       Consumer Loan| 22657|\n",
      "|        Student loan| 16425|\n",
      "|                null| 11200|\n",
      "|               Inc.\"|  5235|\n",
      "|         Payday loan|  4145|\n",
      "|     Money transfers|  4072|\n",
      "|        Prepaid card|  2638|\n",
      "|          collection|  2056|\n",
      "|            payments|  1940|\n",
      "|                LLC\"|  1485|\n",
      "|         health club|  1445|\n",
      "|             closing|   906|\n",
      "|Other financial s...|   622|\n",
      "|                Inc\"|   446|\n",
      "|                XXXX|   396|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Runtime:  4.1099999994 s\n"
     ]
    }
   ],
   "source": [
    "start = os.times()[-1]\n",
    "\n",
    "col_name = \"Product\"\n",
    "\n",
    "# Calculate histogram of string column\n",
    "col_dist_product = ( df.groupby(col_name)                 # aggregate data by column value\n",
    "                    .count()                           # count number of rows for each column value\n",
    "                    .sort(\"count\", ascending=False) )  # sort column value in the descending order\n",
    "\n",
    "# print out distribution\n",
    "col_dist_product.show()\n",
    "\n",
    "end = os.times()[-1]\n",
    "print \"Runtime: \", end - start , \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|         Sub-product| count|\n",
      "+--------------------+------+\n",
      "|                    |171524|\n",
      "|      Other mortgage| 76490|\n",
      "|Conventional fixe...| 59380|\n",
      "|    Checking account| 47133|\n",
      "|Other (i.e. phone...| 31703|\n",
      "|       I do not know| 22674|\n",
      "|         Credit card| 21955|\n",
      "|Conventional adju...| 21652|\n",
      "|        FHA mortgage| 20066|\n",
      "|Non-federal stude...| 18661|\n",
      "|                null| 18259|\n",
      "|             Medical| 14362|\n",
      "|        Vehicle loan| 13025|\n",
      "|Other bank produc...| 12040|\n",
      "|         Payday loan| 10148|\n",
      "|Home equity loan ...|  9408|\n",
      "|    Installment loan|  5821|\n",
      "|     Savings account|  4141|\n",
      "|         VA mortgage|  3987|\n",
      "|            Mortgage|  3739|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Runtime:  3.86000000313 s\n"
     ]
    }
   ],
   "source": [
    "start = os.times()[-1]\n",
    "\n",
    "col_name = \"Sub-product\"\n",
    "\n",
    "# Calculate histogram of string column\n",
    "col_dist_sproduct = ( df.groupby(col_name)                 # aggregate data by column value\n",
    "                    .count()                           # count number of rows for each column value\n",
    "                    .sort(\"count\", ascending=False) )  # sort column value in the descending order\n",
    "\n",
    "# print out distribution\n",
    "col_dist_sproduct.show()\n",
    "\n",
    "end = os.times()[-1]\n",
    "print \"Runtime: \", end - start , \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------+\n",
      "|Consumer complaint narrative| count|\n",
      "+----------------------------+------+\n",
      "|                            |510604|\n",
      "|                        null| 27961|\n",
      "|            Consent provided|  5693|\n",
      "|        Consent not provided|  1439|\n",
      "|        Company chooses n...|  1205|\n",
      "|                         N/A|   648|\n",
      "|        Company has respo...|   434|\n",
      "|        Company believes ...|   414|\n",
      "|        Closed with expla...|   171|\n",
      "|                        XXXX|    88|\n",
      "|        Company believes ...|    67|\n",
      "|              Older American|    61|\n",
      "|        \"I am filing this...|    47|\n",
      "|                       Other|    47|\n",
      "|        This company cont...|    45|\n",
      "|                   XXXX XXXX|    44|\n",
      "|        Company believes ...|    34|\n",
      "|               Servicemember|    32|\n",
      "|        Closed with non-m...|    32|\n",
      "|        \"I am filing this...|    32|\n",
      "+----------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Runtime:  37.0500000007 s\n"
     ]
    }
   ],
   "source": [
    "start = os.times()[-1]\n",
    "\n",
    "col_name = \"Consumer complaint narrative\"\n",
    "\n",
    "# Calculate histogram of string column\n",
    "col_dist_issue = ( df.groupby(col_name)                 # aggregate data by column value\n",
    "                    .count()                           # count number of rows for each column value\n",
    "                    .sort(\"count\", ascending=False) )  # sort column value in the descending order\n",
    "\n",
    "# print out distribution\n",
    "col_dist_issue.show()\n",
    "\n",
    "end = os.times()[-1]\n",
    "print \"Runtime: \", end - start , \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bank of America</td>\n",
       "      <td>56529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wells Fargo &amp; Company</td>\n",
       "      <td>42597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>37747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Equifax</td>\n",
       "      <td>34491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JPMorgan Chase &amp; Co.</td>\n",
       "      <td>34471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Experian</td>\n",
       "      <td>33137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TransUnion Intermediate Holdings, Inc.</td>\n",
       "      <td>27571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Citibank</td>\n",
       "      <td>26793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ocwen</td>\n",
       "      <td>21005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Capital One</td>\n",
       "      <td>15937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Nationstar Mortgage</td>\n",
       "      <td>13315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Synchrony Financial</td>\n",
       "      <td>9751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>U.S. Bancorp</td>\n",
       "      <td>9694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ditech Financial LLC</td>\n",
       "      <td>8950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Navient Solutions, Inc.</td>\n",
       "      <td>8096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>PNC Bank N.A.</td>\n",
       "      <td>7121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Encore Capital Group</td>\n",
       "      <td>6372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>HSBC North America Holdings Inc.</td>\n",
       "      <td>6252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Amex</td>\n",
       "      <td>5911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SunTrust Banks, Inc.</td>\n",
       "      <td>4951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Discover</td>\n",
       "      <td>4837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>TD Bank US Holding Company</td>\n",
       "      <td>4647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Select Portfolio Servicing, Inc</td>\n",
       "      <td>4449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Portfolio Recovery Associates, Inc.</td>\n",
       "      <td>3499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Citizens Financial Group, Inc.</td>\n",
       "      <td>3165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Fifth Third Financial Corporation</td>\n",
       "      <td>2969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Seterus, Inc.</td>\n",
       "      <td>2863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Barclays PLC</td>\n",
       "      <td>2839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ERC</td>\n",
       "      <td>2818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>BB&amp;T Financial</td>\n",
       "      <td>2598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15280</th>\n",
       "      <td>this logic is severely flawed as the primary ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15281</th>\n",
       "      <td>which I had no clue what BOA address the rele...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15282</th>\n",
       "      <td>many other issues. I complain with rewards wi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15283</th>\n",
       "      <td>so XXXX XXXX informed us that we had to use X...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15284</th>\n",
       "      <td>a payment was made that resolved this issue. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15285</th>\n",
       "      <td>GET A RATE LLC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15286</th>\n",
       "      <td>showing another account balance amount that d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15287</th>\n",
       "      <td>was attributed to me alone on the loan applic...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15288</th>\n",
       "      <td>or was it just included in one of the impleme...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15289</th>\n",
       "      <td>painting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15290</th>\n",
       "      <td>thinking we had {$85000.00} in equity</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15291</th>\n",
       "      <td>Allocated Business Management, LLC.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15292</th>\n",
       "      <td>I pointed out to him that the Bank 's ACCOUNT...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15293</th>\n",
       "      <td>20774</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15294</th>\n",
       "      <td>shortly after purchasing a home</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15295</th>\n",
       "      <td>missed ( only XXXX )</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15296</th>\n",
       "      <td>and does not provide any resource to speak wi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15297</th>\n",
       "      <td>I am not given nor ask for any credit from th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15298</th>\n",
       "      <td>I was told my PMI carrier was XXXX XXXX. I le...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15299</th>\n",
       "      <td>33704</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15300</th>\n",
       "      <td>no pending overdraw</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15301</th>\n",
       "      <td>the back building completely</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15302</th>\n",
       "      <td>as I did in the past</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15303</th>\n",
       "      <td>serving me at my job and \"\" embarrassing me a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15304</th>\n",
       "      <td>lied</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15305</th>\n",
       "      <td>a financial services company</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15306</th>\n",
       "      <td>CORRESPONDENCE AND BANK STATEMENTS\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15307</th>\n",
       "      <td>2016 Bank of America confirmed that they have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15308</th>\n",
       "      <td>there was only woman working behind the count...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15309</th>\n",
       "      <td>RevCrest, Inc.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15310 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Company  count\n",
       "0                                        Bank of America  56529\n",
       "1                                  Wells Fargo & Company  42597\n",
       "2                                                   None  37747\n",
       "3                                                Equifax  34491\n",
       "4                                   JPMorgan Chase & Co.  34471\n",
       "5                                               Experian  33137\n",
       "6                 TransUnion Intermediate Holdings, Inc.  27571\n",
       "7                                               Citibank  26793\n",
       "8                                                  Ocwen  21005\n",
       "9                                            Capital One  15937\n",
       "10                                   Nationstar Mortgage  13315\n",
       "11                                   Synchrony Financial   9751\n",
       "12                                          U.S. Bancorp   9694\n",
       "13                                  Ditech Financial LLC   8950\n",
       "14                               Navient Solutions, Inc.   8096\n",
       "15                                         PNC Bank N.A.   7121\n",
       "16                                  Encore Capital Group   6372\n",
       "17                      HSBC North America Holdings Inc.   6252\n",
       "18                                                  Amex   5911\n",
       "19                                  SunTrust Banks, Inc.   4951\n",
       "20                                              Discover   4837\n",
       "21                            TD Bank US Holding Company   4647\n",
       "22                       Select Portfolio Servicing, Inc   4449\n",
       "23                   Portfolio Recovery Associates, Inc.   3499\n",
       "24                        Citizens Financial Group, Inc.   3165\n",
       "25                     Fifth Third Financial Corporation   2969\n",
       "26                                         Seterus, Inc.   2863\n",
       "27                                          Barclays PLC   2839\n",
       "28                                                   ERC   2818\n",
       "29                                        BB&T Financial   2598\n",
       "...                                                  ...    ...\n",
       "15280   this logic is severely flawed as the primary ...      1\n",
       "15281   which I had no clue what BOA address the rele...      1\n",
       "15282   many other issues. I complain with rewards wi...      1\n",
       "15283   so XXXX XXXX informed us that we had to use X...      1\n",
       "15284   a payment was made that resolved this issue. ...      1\n",
       "15285                                     GET A RATE LLC      1\n",
       "15286   showing another account balance amount that d...      1\n",
       "15287   was attributed to me alone on the loan applic...      1\n",
       "15288   or was it just included in one of the impleme...      1\n",
       "15289                                           painting      1\n",
       "15290              thinking we had {$85000.00} in equity      1\n",
       "15291                Allocated Business Management, LLC.      1\n",
       "15292   I pointed out to him that the Bank 's ACCOUNT...      1\n",
       "15293                                              20774      1\n",
       "15294                    shortly after purchasing a home      1\n",
       "15295                               missed ( only XXXX )      1\n",
       "15296   and does not provide any resource to speak wi...      1\n",
       "15297   I am not given nor ask for any credit from th...      1\n",
       "15298   I was told my PMI carrier was XXXX XXXX. I le...      1\n",
       "15299                                              33704      1\n",
       "15300                                no pending overdraw      1\n",
       "15301                       the back building completely      1\n",
       "15302                               as I did in the past      1\n",
       "15303   serving me at my job and \"\" embarrassing me a...      1\n",
       "15304                                               lied      1\n",
       "15305                       a financial services company      1\n",
       "15306                CORRESPONDENCE AND BANK STATEMENTS\"      1\n",
       "15307   2016 Bank of America confirmed that they have...      1\n",
       "15308   there was only woman working behind the count...      1\n",
       "15309                                     RevCrest, Inc.      1\n",
       "\n",
       "[15310 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to Pandas dataframe so that we can generate plots\n",
    "col_dist_company_pd = col_dist_company.toPandas()\n",
    "col_dist_company_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8190efabd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAHJCAYAAACMkO4bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRAAAAAJJREFUOyqb8ptUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81b80ff350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "col_dist_head = col_dist_company_pd.head(20)\n",
    "col_dist_head.plot(x='Company', y='count', kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|State|count|\n",
      "+-----+-----+\n",
      "|   CA|83854|\n",
      "|   FL|55645|\n",
      "|   TX|43148|\n",
      "| null|40433|\n",
      "|   NY|39611|\n",
      "|   GA|25667|\n",
      "|   NJ|23255|\n",
      "|   PA|20620|\n",
      "|   IL|20486|\n",
      "|   VA|18687|\n",
      "|   MD|18246|\n",
      "|   OH|17824|\n",
      "|   NC|15922|\n",
      "|   MI|14838|\n",
      "|   AZ|12660|\n",
      "|   WA|11850|\n",
      "|   MA|11335|\n",
      "|   CO| 9842|\n",
      "|   TN| 8792|\n",
      "|   MO| 7430|\n",
      "+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Runtime:  3.51000000164 s\n"
     ]
    }
   ],
   "source": [
    "start = os.times()[-1]\n",
    "\n",
    "col_name = \"State\"\n",
    "\n",
    "# Calculate histogram of string column\n",
    "col_dist_state = ( df.groupby(col_name)                 # aggregate data by column value\n",
    "                    .count()                           # count number of rows for each column value\n",
    "                    .sort(\"count\", ascending=False) )  # sort column value in the descending order\n",
    "\n",
    "# print out distribution\n",
    "col_dist_state.show()\n",
    "\n",
    "end = os.times()[-1]\n",
    "print \"Runtime: \", end - start , \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to Pandas dataframe so that we can generate plots\n",
    "col_dist_state_pd = col_dist_state.toPandas()\n",
    "state_head = col_dist_state_pd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11917\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>117XX</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>921XX</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>336XX</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>328XX</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>330XX</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>207XX</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>200XX</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>850XX</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>080XX</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Experian</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>913XX</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>06/07/2016</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>06/10/2016</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>322XX</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>33173</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Servicemember</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>PW</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>021XX</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>AA</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             State  count\n",
       "100          117XX     13\n",
       "101          921XX     12\n",
       "102           2016     12\n",
       "103          336XX     12\n",
       "104          328XX     12\n",
       "105          330XX     12\n",
       "106          207XX     12\n",
       "107          200XX     12\n",
       "108          850XX     11\n",
       "109          080XX     11\n",
       "110       Experian     11\n",
       "111          913XX     11\n",
       "112     06/07/2016     11\n",
       "113     06/10/2016     11\n",
       "114          322XX     11\n",
       "115          33173     11\n",
       "116  Servicemember     11\n",
       "117             PW     11\n",
       "118          021XX     11\n",
       "119             AA     10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data quality is awful\n",
    "print(len(col_dist_state_pd))\n",
    "col_dist_state_pd[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "desired_states = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA,\" \n",
    "                  \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \n",
    "                  \"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \n",
    "                  \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport simplemapplot\\n\\nexample_colors = [\"#FC8D59\",\"#FFFFBF\",\"#99D594\"]\\ncountry_value = {\"us\":1, \"au\":2, \"gb\":0}\\nmake_world_country_map(data=country_value, colors=example_colors)\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import simplemapplot\n",
    "\n",
    "example_colors = [\"#FC8D59\",\"#FFFFBF\",\"#99D594\"]\n",
    "country_value = {\"us\":1, \"au\":2, \"gb\":0}\n",
    "make_world_country_map(data=country_value, colors=example_colors)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states3 = col_dist_state_pd[col_dist_state_pd['State'].isin(desired_states)]\n",
    "#col_dist_state_pd[\"State\"] = desired_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f8190a3f610>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEWCAYAAABv+EDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKygnADOzgnICMDMrqP8PdB0AbZnMG0QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8190e12250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "states_head = states3.head(20)\n",
    "states_head.plot(x='State', y='count', kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import matplotlib.pyplot as plt\\nfrom mpl_toolkits.basemap import Basemap\\nfrom matplotlib.patches import Polygon\\n\\n# create the map\\nmap = Basemap(llcrnrlon=-119,llcrnrlat=22,urcrnrlon=-64,urcrnrlat=49,\\n        projection='lcc',lat_1=33,lat_2=45,lon_0=-95)\\n\\n# load the shapefile, use the name 'states'\\nmap.readshapefile('st99_d00', name='states', drawbounds=True)\\n\\n# collect the state names from the shapefile attributes so we can\\n# look up the shape obect for a state by it's name\\nstate_names = []\\nfor shape_dict in map.states_info:\\n    state_names.append(shape_dict['NAME'])\\n\\nax = plt.gca() # get current axes instance\\n\\n# get Texas and draw the filled polygon\\nseg = map.states[state_names.index('Texas')]\\npoly = Polygon(seg, facecolor='red',edgecolor='red')\\nax.add_patch(poly)\\n\\nplt.show()\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "# create the map\n",
    "map = Basemap(llcrnrlon=-119,llcrnrlat=22,urcrnrlon=-64,urcrnrlat=49,\n",
    "        projection='lcc',lat_1=33,lat_2=45,lon_0=-95)\n",
    "\n",
    "# load the shapefile, use the name 'states'\n",
    "map.readshapefile('st99_d00', name='states', drawbounds=True)\n",
    "\n",
    "# collect the state names from the shapefile attributes so we can\n",
    "# look up the shape obect for a state by it's name\n",
    "state_names = []\n",
    "for shape_dict in map.states_info:\n",
    "    state_names.append(shape_dict['NAME'])\n",
    "\n",
    "ax = plt.gca() # get current axes instance\n",
    "\n",
    "# get Texas and draw the filled polygon\n",
    "seg = map.states[state_names.index('Texas')]\n",
    "poly = Polygon(seg, facecolor='red',edgecolor='red')\n",
    "ax.add_patch(poly)\n",
    "\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-25-233ef7eb7eaf>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-25-233ef7eb7eaf>\"\u001b[0;36m, line \u001b[0;32m24\u001b[0m\n\u001b[0;31m    '''\u001b[0m\n\u001b[0m       \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "#Swap county data for state data, reset map\n",
    "import vincent\n",
    "vis = vincent.Map\n",
    "\n",
    "#vis = vincent.Map(width=1000, height=800)\n",
    "#Add the US county data and a new line color\n",
    "vis.geo_data(projection='albersUsa', scale=1000, counties=county_geo)\n",
    "vis + ('2B4ECF', 'marks', 0, 'properties', 'enter', 'stroke', 'value')\n",
    "\n",
    "#Add the state data, remove the fill, write Vega spec output to JSON\n",
    "vis.geo_data(states=state_geo)\n",
    "vis - ('fill', 'marks', 1, 'properties', 'enter')\n",
    "vis.to_json(path)\n",
    "\n",
    "'''\n",
    "vis.tabular_data(col_dist_state_pd, columns=['State', 'count'])\n",
    "vis.geo_data(bind_data='data.id', reset=True, states=state_geo)\n",
    "vis.update_map(scale=1000, projection='albersUsa')\n",
    "vis + (['#c9cedb', '#0b0d11'], 'scales', 0, 'range')\n",
    "vis.to_json(path)'''\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:#6699FF\">Timestamp Conversion in Progress</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|Date received|\n",
      "+-------------+\n",
      "|   07/29/2013|\n",
      "|   07/29/2013|\n",
      "|   07/29/2013|\n",
      "|   07/29/2013|\n",
      "|   07/29/2013|\n",
      "|   07/29/2013|\n",
      "|   07/29/2013|\n",
      "|   07/29/2013|\n",
      "|   07/29/2013|\n",
      "|   07/29/2013|\n",
      "+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Date received\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|       Date received|count|\n",
      "+--------------------+-----+\n",
      "|          06/26/2014|  916|\n",
      "|          07/06/2016|  875|\n",
      "|          08/27/2015|  874|\n",
      "|          08/26/2015|  833|\n",
      "|          07/13/2016|  785|\n",
      "|          08/12/2015|  761|\n",
      "|          05/11/2016|  759|\n",
      "|          05/25/2016|  748|\n",
      "|          07/08/2015|  746|\n",
      "|\",Company chooses...|  740|\n",
      "|          05/03/2016|  718|\n",
      "|          06/28/2016|  706|\n",
      "|          06/08/2016|  706|\n",
      "|          03/26/2014|  704|\n",
      "|          04/23/2014|  702|\n",
      "|          06/29/2016|  700|\n",
      "|          07/07/2016|  700|\n",
      "|          05/10/2016|  699|\n",
      "|          04/12/2016|  697|\n",
      "|          10/14/2015|  696|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Runtime:  19.6199999973 s\n"
     ]
    }
   ],
   "source": [
    "start = os.times()[-1]\n",
    "\n",
    "col_name = \"Date received\"\n",
    "\n",
    "# Calculate histogram of string column\n",
    "col_dist_date = ( df.groupby(col_name)                 # aggregate data by column value\n",
    "                    .count()                           # count number of rows for each column value\n",
    "                    .sort(\"count\", ascending=False) )  # sort column value in the descending order\n",
    "\n",
    "# print out distribution\n",
    "col_dist_date.show()\n",
    "\n",
    "end = os.times()[-1]\n",
    "print \"Runtime: \", end - start , \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_dist_date_pd = col_dist_date.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date received</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>06/26/2014</td>\n",
       "      <td>916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>07/06/2016</td>\n",
       "      <td>875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08/27/2015</td>\n",
       "      <td>874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08/26/2015</td>\n",
       "      <td>833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07/13/2016</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>08/12/2015</td>\n",
       "      <td>761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>05/11/2016</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>05/25/2016</td>\n",
       "      <td>748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>07/08/2015</td>\n",
       "      <td>746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\",Company chooses not to provide a public resp...</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>05/03/2016</td>\n",
       "      <td>718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>06/28/2016</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>06/08/2016</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>03/26/2014</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>04/23/2014</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>06/29/2016</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>07/07/2016</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>05/10/2016</td>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>04/12/2016</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10/14/2015</td>\n",
       "      <td>696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>07/19/2016</td>\n",
       "      <td>694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>02/23/2016</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>05/18/2016</td>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>04/30/2014</td>\n",
       "      <td>687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>01/26/2016</td>\n",
       "      <td>687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>02/26/2014</td>\n",
       "      <td>687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>07/09/2015</td>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>03/19/2015</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>04/26/2016</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>02/21/2014</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41196</th>\n",
       "      <td>These requests for accommodations have been de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41197</th>\n",
       "      <td>During XXXX 2015</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41198</th>\n",
       "      <td>\",,United PanAm Financial Corp.,MO,633XX,,Cons...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41199</th>\n",
       "      <td>\",,Equifax,MD,207XX,,Consent provided,Web,02/1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41200</th>\n",
       "      <td>XXXX XXXX : Learned from funding bank that the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41201</th>\n",
       "      <td>I had a {$0.00} balance since XXXX/XXXX/16 and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41202</th>\n",
       "      <td>Here are our options : 1. If we stay in our ho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41203</th>\n",
       "      <td>XXXX XXXX XXXX XXXX am requesting investigatio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41204</th>\n",
       "      <td>Barclay Account number ending # XXXX was link ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41205</th>\n",
       "      <td>If I was one of the doctors offices that you w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41206</th>\n",
       "      <td>\",,JPMorgan Chase &amp; Co.,WA,982XX,,Consent prov...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41207</th>\n",
       "      <td>Neither I nor the payer were notified of the f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41208</th>\n",
       "      <td>\",Company has responded to the consumer and th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41209</th>\n",
       "      <td>XXXX I then figured out another issue that con...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41210</th>\n",
       "      <td>\",,HSBC North America Holdings Inc.,AR,722XX,,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41211</th>\n",
       "      <td>i. No handoff from prior servicer - When I cal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41212</th>\n",
       "      <td>5 ) \"\" customer '' means any person or authori...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41213</th>\n",
       "      <td>\",Company has responded to the consumer and th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41214</th>\n",
       "      <td>I received a third electronic letter from Citi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41215</th>\n",
       "      <td>I requested : 1. That American Express close t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41216</th>\n",
       "      <td>I further explained to XXXX XXXX that I had no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41217</th>\n",
       "      <td>o BOA sent me a letter of intent to Accelerate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41218</th>\n",
       "      <td>XXXX. Certain employees refuse to provide a lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41219</th>\n",
       "      <td>XXXX XXXX XXXX XXXX This is double jeopardy - ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41220</th>\n",
       "      <td>\"\" XXXX XXXX 's Home Value Calibrator Home Val...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41221</th>\n",
       "      <td>I also do not want to hear their mumbo jumbo a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41222</th>\n",
       "      <td>Despite my good faith request that the tape of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41223</th>\n",
       "      <td>If the individual making the affidavit had rev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41224</th>\n",
       "      <td>In addition to the debit from my account havin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41225</th>\n",
       "      <td>It is my position that both inquiries should b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41226 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Date received  count\n",
       "0                                             06/26/2014    916\n",
       "1                                             07/06/2016    875\n",
       "2                                             08/27/2015    874\n",
       "3                                             08/26/2015    833\n",
       "4                                             07/13/2016    785\n",
       "5                                             08/12/2015    761\n",
       "6                                             05/11/2016    759\n",
       "7                                             05/25/2016    748\n",
       "8                                             07/08/2015    746\n",
       "9      \",Company chooses not to provide a public resp...    740\n",
       "10                                            05/03/2016    718\n",
       "11                                            06/28/2016    706\n",
       "12                                            06/08/2016    706\n",
       "13                                            03/26/2014    704\n",
       "14                                            04/23/2014    702\n",
       "15                                            06/29/2016    700\n",
       "16                                            07/07/2016    700\n",
       "17                                            05/10/2016    699\n",
       "18                                            04/12/2016    697\n",
       "19                                            10/14/2015    696\n",
       "20                                            07/19/2016    694\n",
       "21                                            02/23/2016    690\n",
       "22                                            05/18/2016    689\n",
       "23                                            04/30/2014    687\n",
       "24                                            01/26/2016    687\n",
       "25                                            02/26/2014    687\n",
       "26                                            07/09/2015    686\n",
       "27                                            03/19/2015    684\n",
       "28                                            04/26/2016    684\n",
       "29                                            02/21/2014    683\n",
       "...                                                  ...    ...\n",
       "41196  These requests for accommodations have been de...      1\n",
       "41197                                   During XXXX 2015      1\n",
       "41198  \",,United PanAm Financial Corp.,MO,633XX,,Cons...      1\n",
       "41199  \",,Equifax,MD,207XX,,Consent provided,Web,02/1...      1\n",
       "41200  XXXX XXXX : Learned from funding bank that the...      1\n",
       "41201  I had a {$0.00} balance since XXXX/XXXX/16 and...      1\n",
       "41202  Here are our options : 1. If we stay in our ho...      1\n",
       "41203  XXXX XXXX XXXX XXXX am requesting investigatio...      1\n",
       "41204  Barclay Account number ending # XXXX was link ...      1\n",
       "41205  If I was one of the doctors offices that you w...      1\n",
       "41206  \",,JPMorgan Chase & Co.,WA,982XX,,Consent prov...      1\n",
       "41207  Neither I nor the payer were notified of the f...      1\n",
       "41208  \",Company has responded to the consumer and th...      1\n",
       "41209  XXXX I then figured out another issue that con...      1\n",
       "41210  \",,HSBC North America Holdings Inc.,AR,722XX,,...      1\n",
       "41211  i. No handoff from prior servicer - When I cal...      1\n",
       "41212  5 ) \"\" customer '' means any person or authori...      1\n",
       "41213  \",Company has responded to the consumer and th...      1\n",
       "41214  I received a third electronic letter from Citi...      1\n",
       "41215  I requested : 1. That American Express close t...      1\n",
       "41216  I further explained to XXXX XXXX that I had no...      1\n",
       "41217  o BOA sent me a letter of intent to Accelerate...      1\n",
       "41218  XXXX. Certain employees refuse to provide a lo...      1\n",
       "41219  XXXX XXXX XXXX XXXX This is double jeopardy - ...      1\n",
       "41220  \"\" XXXX XXXX 's Home Value Calibrator Home Val...      1\n",
       "41221  I also do not want to hear their mumbo jumbo a...      1\n",
       "41222  Despite my good faith request that the tape of...      1\n",
       "41223  If the individual making the affidavit had rev...      1\n",
       "41224  In addition to the debit from my account havin...      1\n",
       "41225  It is my position that both inquiries should b...      1\n",
       "\n",
       "[41226 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_dist_date_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mask = (col_dist_date_pd['Date received'].str.len() == 10)\n",
    "col_dist_date_pd = col_dist_date_pd.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-17-f4fc7fca4a3d>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-f4fc7fca4a3d>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    return row['Date received'] == row['Date received'].encode('ascii', 'ignore').decode('ascii')\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "#col_dist_date_pd = col_dist_date_pd(encoding ='utf-8')\n",
    "for row in col_dist_date_pd:\n",
    "    return row['Date received'] == row['Date received'].encode('ascii', 'ignore').decode('ascii')\n",
    "#yourstring = yourstring.encode('ascii', 'ignore').decode('ascii')\n",
    "#col_dist_date_pd['Date received'] = col_dist_date_pd['Date received'].astype(str).str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date received</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>06/26/2014</td>\n",
       "      <td>916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>07/06/2016</td>\n",
       "      <td>875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08/27/2015</td>\n",
       "      <td>874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08/26/2015</td>\n",
       "      <td>833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>07/13/2016</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>08/12/2015</td>\n",
       "      <td>761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>05/11/2016</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>05/25/2016</td>\n",
       "      <td>748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>07/08/2015</td>\n",
       "      <td>746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>05/03/2016</td>\n",
       "      <td>718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>06/28/2016</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>06/08/2016</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>03/26/2014</td>\n",
       "      <td>704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>04/23/2014</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>06/29/2016</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>07/07/2016</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>05/10/2016</td>\n",
       "      <td>699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>04/12/2016</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10/14/2015</td>\n",
       "      <td>696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>07/19/2016</td>\n",
       "      <td>694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>02/23/2016</td>\n",
       "      <td>690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>05/18/2016</td>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>04/30/2014</td>\n",
       "      <td>687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>01/26/2016</td>\n",
       "      <td>687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>02/26/2014</td>\n",
       "      <td>687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>07/09/2015</td>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>03/19/2015</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>04/26/2016</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>02/21/2014</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>02/27/2014</td>\n",
       "      <td>678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>05/06/2012</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1737</th>\n",
       "      <td>06/03/2012</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1739</th>\n",
       "      <td>12/17/2011</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>09/03/2012</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1743</th>\n",
       "      <td>01/02/2012</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1744</th>\n",
       "      <td>04/08/2012</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>05/12/2012</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>12/03/2011</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>01/29/2012</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>05/28/2012</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>08/05/2012</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1753</th>\n",
       "      <td>01/15/2012</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1754</th>\n",
       "      <td>12/31/2011</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1756</th>\n",
       "      <td>04/15/2012</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>08/08/2016</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>03/18/2012</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>12/04/2011</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>02/19/2012</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1774</th>\n",
       "      <td>12/26/2011</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>09/02/2012</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>05/27/2012</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>05/13/2012</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>12/18/2011</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>08/06/2016</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1808</th>\n",
       "      <td>01/01/2012</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1811</th>\n",
       "      <td>08/07/2016</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1836</th>\n",
       "      <td>08/09/2016</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>12/24/2011</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864</th>\n",
       "      <td>12/25/2011</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>03/09/2014</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1714 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Date received  count\n",
       "0       06/26/2014    916\n",
       "1       07/06/2016    875\n",
       "2       08/27/2015    874\n",
       "3       08/26/2015    833\n",
       "4       07/13/2016    785\n",
       "5       08/12/2015    761\n",
       "6       05/11/2016    759\n",
       "7       05/25/2016    748\n",
       "8       07/08/2015    746\n",
       "10      05/03/2016    718\n",
       "11      06/28/2016    706\n",
       "12      06/08/2016    706\n",
       "13      03/26/2014    704\n",
       "14      04/23/2014    702\n",
       "15      06/29/2016    700\n",
       "16      07/07/2016    700\n",
       "17      05/10/2016    699\n",
       "18      04/12/2016    697\n",
       "19      10/14/2015    696\n",
       "20      07/19/2016    694\n",
       "21      02/23/2016    690\n",
       "22      05/18/2016    689\n",
       "23      04/30/2014    687\n",
       "24      01/26/2016    687\n",
       "25      02/26/2014    687\n",
       "26      07/09/2015    686\n",
       "27      03/19/2015    684\n",
       "28      04/26/2016    684\n",
       "29      02/21/2014    683\n",
       "30      02/27/2014    678\n",
       "...            ...    ...\n",
       "1735    05/06/2012     29\n",
       "1737    06/03/2012     28\n",
       "1739    12/17/2011     28\n",
       "1741    09/03/2012     27\n",
       "1743    01/02/2012     27\n",
       "1744    04/08/2012     27\n",
       "1746    05/12/2012     26\n",
       "1747    12/03/2011     26\n",
       "1750    01/29/2012     26\n",
       "1751    05/28/2012     25\n",
       "1752    08/05/2012     25\n",
       "1753    01/15/2012     25\n",
       "1754    12/31/2011     25\n",
       "1756    04/15/2012     25\n",
       "1761    08/08/2016     24\n",
       "1765    03/18/2012     23\n",
       "1770    12/04/2011     22\n",
       "1773    02/19/2012     22\n",
       "1774    12/26/2011     21\n",
       "1776    09/02/2012     20\n",
       "1780    05/27/2012     19\n",
       "1781    05/13/2012     18\n",
       "1790    12/18/2011     16\n",
       "1799    08/06/2016     15\n",
       "1808    01/01/2012     14\n",
       "1811    08/07/2016     13\n",
       "1836    08/09/2016     11\n",
       "1843    12/24/2011     11\n",
       "1864    12/25/2011     10\n",
       "2033    03/09/2014      5\n",
       "\n",
       "[1714 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for row in col_dist_date_pd:\n",
    "#if col_dist_date_pd['Date received'].astype(str).str[2]==\"/\":\n",
    "#    new_date_count.apppend(i)\n",
    "\n",
    "#col_dist_date_pd['Date received'].astype(str).str[2:].str.contains('/')\n",
    "\n",
    "new_date_count = col_dist_date_pd[col_dist_date_pd['Date received'].astype(str).str[2:].str.contains('/')]\n",
    "#new_date_count = col_dist_date_pd[col_dist_date_pd['Date received'].astype(str).str[5:].str.contains('/')]\n",
    "new_date_count = new_date_count[new_date_count['Date received'].astype(str).str.contains(\"X\")== False]\n",
    "new_date_count\n",
    "#slash_index = col_dist_date_pd['Date received'].str[2]\n",
    "#slash_index\n",
    "# 1714"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unconverted data remains: 14",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-79da7b418993>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_date_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date received'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_date_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date received'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%m/%d/%y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#new_date_count[new_date_count.Date received.str.contains(\"X\") == False]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#new_date_count = col_dist_date_pd[col_dist_date_pd['Date received'].astype(str).str[2:].str.contains('/')]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python2.7/site-packages/pandas/util/decorators.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python2.7/site-packages/pandas/tseries/tools.pyc\u001b[0m in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, box, format, exact, coerce, unit, infer_datetime_format)\u001b[0m\n\u001b[1;32m    289\u001b[0m                         \u001b[0myearfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myearfirst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                         \u001b[0mutc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mutc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                         unit=unit, infer_datetime_format=infer_datetime_format)\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python2.7/site-packages/pandas/tseries/tools.pyc\u001b[0m in \u001b[0;36m_to_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, box, format, exact, unit, freq, infer_datetime_format)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mABCDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMutableMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python2.7/site-packages/pandas/tseries/tools.pyc\u001b[0m in \u001b[0;36m_convert_listlike\u001b[0;34m(arg, box, format, name)\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mDatetimeIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_simple_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unconverted data remains: 14"
     ]
    }
   ],
   "source": [
    "new_date_count['Date received'] =  pd.to_datetime(new_date_count['Date received'], format='%m/%d/%y')\n",
    "\n",
    "#new_date_count[new_date_count.Date received.str.contains(\"X\") == False]\n",
    "\n",
    "#new_date_count = col_dist_date_pd[col_dist_date_pd['Date received'].astype(str).str[2:].str.contains('/')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###### Count number of rows\n",
    "start = os.times()[-1]\n",
    "\n",
    "count_rows = df.count()\n",
    "print \"total number of rows: \", count_rows\n",
    "\n",
    "end = os.times()[-1]\n",
    "print \"Runtime: \", end - start , \"s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"color:#6699FF\">Understand Missing Values</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Null value count in column  Company  is:  37747\n",
      "Runtime:  2.87000000104 s\n"
     ]
    }
   ],
   "source": [
    "# Count number of NA columns by company\n",
    "\n",
    "start = os.times()[-1]\n",
    "\n",
    "# select column\n",
    "col_name = 'Company'\n",
    "\n",
    "# count null value\n",
    "count_null = df.where(df[col_name].isNull()).count() \n",
    "\n",
    "print \"Total Null value count in column \", col_name, \" is: \", count_null\n",
    "\n",
    "end = os.times()[-1]\n",
    "print \"Runtime: \", end - start , \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               Issue|\n",
      "+--------------------+\n",
      "|Communication tac...|\n",
      "|               921XX|\n",
      "| now I know I pai...|\n",
      "| I have years of ...|\n",
      "| only a filing of...|\n",
      "| even after I inf...|\n",
      "| including Truth ...|\n",
      "| there were no in...|\n",
      "| I will be at ris...|\n",
      "|               75007|\n",
      "|               068XX|\n",
      "| the name of thei...|\n",
      "| the only change ...|\n",
      "| not later than 9...|\n",
      "| this has left us...|\n",
      "| and other inform...|\n",
      "| and does n't pro...|\n",
      "| as there are XXX...|\n",
      "| as if I were res...|\n",
      "|               97128|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Number of unique value in column  Issue  is:  11737\n",
      "Runtime:  7.6799999997 s\n"
     ]
    }
   ],
   "source": [
    "start = os.times()[-1]\n",
    "\n",
    "# select column\n",
    "col_name = 'Issue'\n",
    "\n",
    "# list of unique values in dataframe format\n",
    "df_col_unique_value = df.select(col_name).distinct()\n",
    "\n",
    "# print out unique value table\n",
    "df_col_unique_value.show()\n",
    "\n",
    "# collect the list from the dataframe\n",
    "list_col_unique_value = df_col_unique_value.collect()\n",
    "\n",
    "# reformat: extract the value from the list\n",
    "list_col_unique_value = [value[0] for value in list_col_unique_value]\n",
    "\n",
    "# convert from list into pandas dataframe\n",
    "pd_col_unique_value = pd.DataFrame({ col_name : df.columns})\n",
    "\n",
    "'''# save the list in the csv format\n",
    "file_dir = col_name + \".csv\"\n",
    "pd_col_unique_value.to_csv(\"taxi_summary.csv\")\n",
    "print \"Unique list of column \", col_name, \" is now saved in the csv file in directory: \", file_dir'''\n",
    "\n",
    "# count the number of unique value\n",
    "count_unique = len(list_col_unique_value)\n",
    "\n",
    "print \"Number of unique value in column \", col_name, \" is: \", count_unique\n",
    "\n",
    "end = os.times()[-1]\n",
    "print \"Runtime: \", end - start , \"s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o83.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/usr/share/nltk_data/corpora/state_union/1972-Nixon.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:253)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:201)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:281)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:60)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-1511c5262f36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mpos_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \"\"\"\n\u001b[1;32m   1279\u001b[0m         \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m         \u001b[0mtotalParts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2388\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2390\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    311\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o83.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/usr/share/nltk_data/corpora/state_union/1972-Nixon.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:253)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:201)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:281)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:246)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:60)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "# spark-nltk.py\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\"\"\"conf = SparkConf()\n",
    "conf.setMaster('yarn-client')\n",
    "conf.setAppName('spark-nltk')\n",
    "sc = SparkContext(conf=conf)\"\"\"\n",
    "\n",
    "data = sc.textFile('file:///usr/share/nltk_data/corpora/state_union/1972-Nixon.txt')\n",
    "\n",
    "def word_tokenize(x):\n",
    "    import nltk\n",
    "    return nltk.word_tokenize(x)\n",
    "\n",
    "def pos_tag(x):\n",
    "    import nltk\n",
    "    return nltk.pos_tag([x])\n",
    "\n",
    "words = data.flatMap(word_tokenize)\n",
    "print words.take(10)\n",
    "\n",
    "pos_word = words.map(pos_tag)\n",
    "print pos_word.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-756261e5b1c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_libsvm_data.txt\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             raise AttributeError(\n\u001b[0;32m--> 844\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m    845\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "data = nlp_df\n",
    "#data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_libsvm_data.txt\")\n",
    "label = data.map(lambda x: x.label)\n",
    "features = data.map(lambda x: x.features)\n",
    "\n",
    "scaler1 = StandardScaler().fit(features)\n",
    "scaler2 = StandardScaler(withMean=True, withStd=True).fit(features)\n",
    "\n",
    "# data1 will be unit variance.\n",
    "data1 = label.zip(scaler1.transform(features))\n",
    "\n",
    "# Without converting the features into dense vectors, transformation with zero mean will raise\n",
    "# exception on sparse vector.\n",
    "# data2 will be unit variance and zero mean.\n",
    "data2 = label.zip(scaler1.transform(features.map(lambda x: Vectors.dense(x.toArray()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=yarn) created by <module> at /usr/local/lib/python2.7/site-packages/IPython/utils/py3compat.py:289 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-0f964d950c47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Word2Vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#inp = sc2.textFile(\"text8_lines\").map(lambda row: row.split(\" \"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    257\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 259\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    260\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=yarn) created by <module> at /usr/local/lib/python2.7/site-packages/IPython/utils/py3compat.py:289 "
     ]
    }
   ],
   "source": [
    "sc2 = SparkContext(appName='Word2Vec')\n",
    "#inp = sc2.textFile(\"text8_lines\").map(lambda row: row.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "word2vec = Word2Vec()\n",
    "model = word2vec.fit(inp)\n",
    "\n",
    "synonyms = model.findSynonyms('china', 40)\n",
    "\n",
    "for word, cosine_distance in synonyms:\n",
    "    print \"{}: {}\".format(word, cosine_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------+\n",
      "|Consumer complaint narrative| count|\n",
      "+----------------------------+------+\n",
      "|                            |510604|\n",
      "|                        null| 27961|\n",
      "|            Consent provided|  5692|\n",
      "|        Consent not provided|  1439|\n",
      "|        Company chooses n...|  1205|\n",
      "|                         N/A|   648|\n",
      "|        Company has respo...|   434|\n",
      "|        Company believes ...|   414|\n",
      "|        Closed with expla...|   171|\n",
      "|                        XXXX|    88|\n",
      "|        Company believes ...|    67|\n",
      "|              Older American|    61|\n",
      "|                       Other|    47|\n",
      "|        \"I am filing this...|    47|\n",
      "|        This company cont...|    45|\n",
      "|                   XXXX XXXX|    44|\n",
      "|        Company believes ...|    34|\n",
      "|               Servicemember|    32|\n",
      "|        Closed with non-m...|    32|\n",
      "|        \"I am filing this...|    32|\n",
      "+----------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Runtime:  57.7199999988 s\n"
     ]
    }
   ],
   "source": [
    "start = os.times()[-1]\n",
    "\n",
    "col_name = \"Consumer complaint narrative\"\n",
    "\n",
    "# Calculate histogram of string column\n",
    "col_dist_complaints = ( df.groupby(col_name)                 # aggregate data by column value\n",
    "                    .count()                           # count number of rows for each column value\n",
    "                    .sort(\"count\", ascending=False) )  # sort column value in the descending order\n",
    "\n",
    "# print out distribution\n",
    "col_dist_complaints.show()\n",
    "\n",
    "end = os.times()[-1]\n",
    "print \"Runtime: \", end - start , \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GroupedData' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-8a254782c3a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# print out distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcol_dist_complaints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GroupedData' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "'''start = os.times()[-1]\n",
    "\n",
    "col_name = \"Consumer complaint narrative\"\n",
    "\n",
    "# Calculate histogram of string column\n",
    "col_dist_complaints = (df.groupby(col_name))\n",
    "\n",
    "# print out distribution\n",
    "col_dist_complaints.show()\n",
    "\n",
    "end = os.times()[-1]\n",
    "print \"Runtime: \", end - start , \"s\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n",
      "|Consumer complaint narrative|\n",
      "+----------------------------+\n",
      "|                            |\n",
      "|                            |\n",
      "|                            |\n",
      "|                            |\n",
      "|                            |\n",
      "|                            |\n",
      "|                            |\n",
      "|                            |\n",
      "|                            |\n",
      "|                            |\n",
      "|                            |\n",
      "|                            |\n",
      "|                            |\n",
      "|                            |\n",
      "|                            |\n",
      "|                            |\n",
      "|                            |\n",
      "|                            |\n",
      "|                            |\n",
      "|                            |\n",
      "+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Runtime:  2.1400000006 s\n"
     ]
    }
   ],
   "source": [
    "start = os.times()[-1]\n",
    "# Select only the \"Issue\", \"Consumer complaint narrative\", and \"State\" columns\n",
    "\n",
    "#nlp_df = df.select(\"Issue\", \"Consumer complaint narrative\", \"State\")\n",
    "nlp_df = df.select(\"Consumer complaint narrative\")\n",
    "nlp_df.show()\n",
    "end = os.times()[-1]\n",
    "print \"Runtime: \", end - start , \"s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------------+-----+\n",
      "|               Issue|Consumer complaint narrative|State|\n",
      "+--------------------+----------------------------+-----+\n",
      "|Using a debit or ...|                            |   CA|\n",
      "|Loan servicing, p...|                            |   CA|\n",
      "|Loan modification...|                            |   CA|\n",
      "|Credit monitoring...|                            |   CA|\n",
      "|Loan servicing, p...|                            |   CA|\n",
      "|Deposits and with...|                            |   CA|\n",
      "|Incorrect informa...|                            |   CA|\n",
      "|Managing the loan...|                            |   CA|\n",
      "|Loan servicing, p...|                            |   CA|\n",
      "|Deposits and with...|                            |   CA|\n",
      "|Loan modification...|                            |   CA|\n",
      "|Application, orig...|                            |   CA|\n",
      "|Loan modification...|                            |   CA|\n",
      "|Loan modification...|                            |   CA|\n",
      "|Loan modification...|                            |   CA|\n",
      "|Loan modification...|                            |   CA|\n",
      "|Loan modification...|                            |   CA|\n",
      "|Loan modification...|                            |   CA|\n",
      "|Cont'd attempts c...|                            |   CA|\n",
      "|Credit determination|                            |   CA|\n",
      "+--------------------+----------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp_ca = nlp_df.filter(nlp_df['State'] == \"CA\")\n",
    "nlp_ca.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = col_dist_complaints\n",
    "tokenizer = Tokenizer(inputCol=\"Consumer complaint narrative\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "for features_label in rescaledData.select(\"features\").take(3):\n",
    "  print(features_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            sentence|\n",
      "+-----+--------------------+\n",
      "|    0|Hi I heard about ...|\n",
      "|    0|I wish Java could...|\n",
      "|    1|Logistic regressi...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceData.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(features=SparseVector(20, {0: 0.6931, 5: 0.6931, 9: 0.2877, 17: 1.3863}), label=0)\n",
      "Row(features=SparseVector(20, {2: 0.6931, 7: 0.6931, 9: 0.863, 13: 0.2877, 15: 0.2877}), label=0)\n",
      "Row(features=SparseVector(20, {4: 0.6931, 6: 0.6931, 13: 0.2877, 15: 0.2877, 18: 0.6931}), label=1)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = sqlContext.createDataFrame([\n",
    "  (0, \"Hi I heard about Spark\"),\n",
    "  (0, \"I wish Java could use case classes\"),\n",
    "  (1, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "for features_label in rescaledData.select(\"features\", \"label\").take(3):\n",
    "  print(features_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(result=DenseVector([0.0075, -0.0373, 0.0178]))\n",
      "Row(result=DenseVector([-0.0173, 0.0307, 0.047]))\n",
      "Row(result=DenseVector([0.1011, -0.0431, 0.0058]))\n"
     ]
    }
   ],
   "source": [
    "# WORD 2 VEC\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = sqlContext.createDataFrame([\n",
    "  (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "  (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "  (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "result = model.transform(documentDF)\n",
    "for feature in result.select(\"result\").take(3):\n",
    "  print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(words=[u'hi', u'i', u'heard', u'about', u'spark'], label=0)\n",
      "Row(words=[u'i', u'wish', u'java', u'could', u'use', u'case', u'classes'], label=1)\n",
      "Row(words=[u'logistic,regression,models,are,neat'], label=2)\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZER - https://spark.apache.org/docs/1.5.1/ml-features.html#tf-idf-hashingtf-and-idf\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "\n",
    "sentenceDataFrame = sqlContext.createDataFrame([\n",
    "  (0, \"Hi I heard about Spark\"),\n",
    "  (1, \"I wish Java could use case classes\"),\n",
    "  (2, \"Logistic,regression,models,are,neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsDataFrame = tokenizer.transform(sentenceDataFrame)\n",
    "for words_label in wordsDataFrame.select(\"words\", \"label\").take(3):\n",
    "  print(words_label)\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(ngrams=[u'Hi I', u'I heard', u'heard about', u'about Spark'], label=0)\n",
      "Row(ngrams=[u'I wish', u'wish Java', u'Java could', u'could use', u'use case', u'case classes'], label=1)\n",
      "Row(ngrams=[u'Logistic regression', u'regression models', u'models are', u'are neat'], label=2)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "wordDataFrame = sqlContext.createDataFrame([\n",
    "  (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n",
    "  (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n",
    "  (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n",
    "], [\"label\", \"words\"])\n",
    "ngram = NGram(inputCol=\"words\", outputCol=\"ngrams\")\n",
    "ngramDataFrame = ngram.transform(wordDataFrame)\n",
    "for ngrams_label in ngramDataFrame.select(\"ngrams\", \"label\").take(3):\n",
    "  print(ngrams_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "continuousDataFrame = sqlContext.createDataFrame([\n",
    "  (0, 0.1),\n",
    "  (1, 0.8),\n",
    "  (2, 0.2)\n",
    "], [\"label\", \"feature\"])\n",
    "binarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n",
    "binarizedDataFrame = binarizer.transform(continuousDataFrame)\n",
    "binarizedFeatures = binarizedDataFrame.select(\"binarized_feature\")\n",
    "for binarized_feature, in binarizedFeatures.collect():\n",
    "  print(binarized_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+-------------+\n",
      "| id|category|categoryIndex|\n",
      "+---+--------+-------------+\n",
      "|  0|       a|          0.0|\n",
      "|  1|       b|          2.0|\n",
      "|  2|       c|          1.0|\n",
      "|  3|       a|          0.0|\n",
      "|  4|       a|          0.0|\n",
      "|  5|       c|          1.0|\n",
      "+---+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "df = sqlContext.createDataFrame(\n",
    "    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n",
    "    [\"id\", \"category\"])\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "df = sqlContext.createDataFrame([\n",
    "  (0, \"a\"),\n",
    "  (1, \"b\"),\n",
    "  (2, \"c\"),\n",
    "  (3, \"a\"),\n",
    "  (4, \"a\"),\n",
    "  (5, \"c\")\n",
    "], [\"id\", \"category\"])\n",
    "\n",
    "stringIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "model = stringIndexer.fit(df)\n",
    "indexed = model.transform(df)\n",
    "encoder = OneHotEncoder(inputCol=\"categoryIndex\", outputCol=\"categoryVec\")\n",
    "encoded = encoder.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|      features|label|\n",
      "+--------------+-----+\n",
      "|[0.0,0.0,18.0]|  1.0|\n",
      "|[0.0,1.0,12.0]|  0.0|\n",
      "|[1.0,0.0,15.0]|  0.0|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "dataset = sqlContext.createDataFrame(\n",
    "    [(7, \"US\", 18, 1.0),\n",
    "     (8, \"CA\", 12, 0.0),\n",
    "     (9, \"NZ\", 15, 0.0)],\n",
    "    [\"id\", \"country\", \"hour\", \"clicked\"])\n",
    "formula = RFormula(\n",
    "    formula=\"clicked ~ country + hour\",\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\")\n",
    "output = formula.fit(dataset).transform(dataset)\n",
    "output.select(\"features\", \"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+--------------------+\n",
      "| id|          words|            features|\n",
      "+---+---------------+--------------------+\n",
      "|  0|      [a, b, c]|(3,[0,1,2],[1.0,1...|\n",
      "|  1|[a, b, b, c, a]|(3,[0,1,2],[2.0,2...|\n",
      "+---+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Input data: Each row is a bag of words with a ID.\n",
    "df = spark.createDataFrame([\n",
    "    (0, \"a b c\".split(\" \")),\n",
    "    (1, \"a b b c a\".split(\" \"))\n",
    "], [\"id\", \"words\"])\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n",
    "model = cv.fit(df)\n",
    "result = model.transform(df)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'outputCol'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-b37d3cc8880a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"words\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0midfModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturizedData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrescaledData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midfModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturizedData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeatures_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrescaledData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'outputCol'"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol=\"words\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "for features_label in rescaledData.select(\"features\", \"label\").take(3):\n",
    "    print(features_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 44.0 failed 4 times, most recent failure: Lost task 0.3 in stage 44.0 (TID 2945, ip-172-31-33-216.ec2.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt1/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000006/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/mnt1/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000006/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 317, in func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 748, in processPartition\n  File \"<ipython-input-58-0a7a6a5e4e0d>\", line 6, in f\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\ufffd' in position 2: ordinal not in range(128)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1911)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:893)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:892)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt1/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000006/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/mnt1/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000006/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 317, in func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 748, in processPartition\n  File \"<ipython-input-58-0a7a6a5e4e0d>\", line 6, in f\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\ufffd' in position 2: ordinal not in range(128)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-0a7a6a5e4e0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#print data.flatMap(f).first()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m '''\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mforeach\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    748\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessPartition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Force evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforeachPartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \"\"\"\n\u001b[0;32m-> 1008\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m         \"\"\"\n\u001b[0;32m--> 999\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \"\"\"\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    311\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 44.0 failed 4 times, most recent failure: Lost task 0.3 in stage 44.0 (TID 2945, ip-172-31-33-216.ec2.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt1/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000006/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/mnt1/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000006/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 317, in func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 748, in processPartition\n  File \"<ipython-input-58-0a7a6a5e4e0d>\", line 6, in f\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\ufffd' in position 2: ordinal not in range(128)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1911)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:893)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:892)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt1/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000006/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/mnt1/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000006/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 2371, in pipeline_func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 317, in func\n  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 748, in processPartition\n  File \"<ipython-input-58-0a7a6a5e4e0d>\", line 6, in f\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\ufffd' in position 2: ordinal not in range(128)\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# input file directory, one single file\n",
    "file_dir = 's3://consumer-complaint-data/trumptext.txt'\n",
    "\n",
    "data = sc.textFile(file_dir)\n",
    "def f(x):\n",
    "    print x\n",
    "data.foreach(f)\n",
    "#print data.flatMap(f).first()\n",
    "'''\n",
    "# load one text file\n",
    "df = (sqlContext.read\n",
    "            .format('com.databricks.spark.csv')\n",
    "            .options(header='true',inferSchema=\"true\",delimiter=\",\") # csv library will infer schema automatically\n",
    "            .load(file_dir))  # specify the file directory here\n",
    "            #.cache()) # cache is optional, if you want to save data in memory'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-4ed5150854b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"ONE TWO\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"THREE FOUR\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "documents = [\"ONE TWO\",\"THREE FOUR\"].map(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TF and IDF are implemented in HashingTF and IDF\n",
    "#HashingTF takes an RDD of list as the input\n",
    "#Each record could be an iterable of strings or other types\n",
    "\n",
    "rdd = nlp_df.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load documents (one per line).\n",
    "documents = rdd.map(lambda line: line.split(\" \"))\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[115] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying HashingTF only needs a single pass to the data\n",
    "#applying IDF needs two passes: first to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "\n",
    "tf.cache()\n",
    "#idf = IDF().fit(tf)\n",
    "#tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'PipelinedRDD' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-69953eab053b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'PipelinedRDD' object does not support indexing"
     ]
    }
   ],
   "source": [
    "print tf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o172.fitIDF.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 37.0 failed 4 times, most recent failure: Lost task 2.3 in stage 37.0 (TID 2923, ip-172-31-33-216.ec2.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000003/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000003/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000003/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-31-8a89e8eb336d>\", line 2, in <lambda>\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000003/pyspark.zip/pyspark/sql/types.py\", line 1484, in __getattr__\n    raise AttributeError(item)\nAttributeError: split\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:910)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:910)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:668)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:281)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1934)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:983)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:965)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1108)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1085)\n\tat org.apache.spark.mllib.feature.IDF.fit(IDF.scala:54)\n\tat org.apache.spark.mllib.feature.IDF.fit(IDF.scala:67)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.fitIDF(PythonMLLibAPI.scala:658)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000003/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000003/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000003/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-31-8a89e8eb336d>\", line 2, in <lambda>\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000003/pyspark.zip/pyspark/sql/types.py\", line 1484, in __getattr__\n    raise AttributeError(item)\nAttributeError: split\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:910)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:910)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:668)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:281)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b607c708f1b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#tfidf = idf.transform(tf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/mllib/feature.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset should be an RDD of term frequency vectors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mjmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fitIDF\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminDocFreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_to_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mIDFModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    311\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o172.fitIDF.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 37.0 failed 4 times, most recent failure: Lost task 2.3 in stage 37.0 (TID 2923, ip-172-31-33-216.ec2.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000003/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000003/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000003/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-31-8a89e8eb336d>\", line 2, in <lambda>\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000003/pyspark.zip/pyspark/sql/types.py\", line 1484, in __getattr__\n    raise AttributeError(item)\nAttributeError: split\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:910)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:910)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:668)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:281)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1934)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:983)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:965)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1108)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1085)\n\tat org.apache.spark.mllib.feature.IDF.fit(IDF.scala:54)\n\tat org.apache.spark.mllib.feature.IDF.fit(IDF.scala:67)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.fitIDF(PythonMLLibAPI.scala:658)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000003/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000003/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000003/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-31-8a89e8eb336d>\", line 2, in <lambda>\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471788503724_0001/container_1471788503724_0001_01_000003/pyspark.zip/pyspark/sql/types.py\", line 1484, in __getattr__\n    raise AttributeError(item)\nAttributeError: split\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:910)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:910)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:668)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:281)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "idf = IDF().fit(tf)\n",
    "#tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o401.fitIDF.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 75.0 failed 4 times, most recent failure: Lost task 0.3 in stage 75.0 (TID 4411, ip-172-31-36-27.ec2.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471272126906_0001/container_1471272126906_0001_01_000022/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471272126906_0001/container_1471272126906_0001_01_000022/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471272126906_0001/container_1471272126906_0001_01_000022/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-90-8a89e8eb336d>\", line 2, in <lambda>\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471272126906_0001/container_1471272126906_0001_01_000022/pyspark.zip/pyspark/sql/types.py\", line 1484, in __getattr__\n    raise AttributeError(item)\nAttributeError: split\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:910)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:910)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:668)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:281)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1934)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:983)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:965)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1108)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1085)\n\tat org.apache.spark.mllib.feature.IDF.fit(IDF.scala:54)\n\tat org.apache.spark.mllib.feature.IDF.fit(IDF.scala:67)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.fitIDF(PythonMLLibAPI.scala:658)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471272126906_0001/container_1471272126906_0001_01_000022/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471272126906_0001/container_1471272126906_0001_01_000022/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471272126906_0001/container_1471272126906_0001_01_000022/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-90-8a89e8eb336d>\", line 2, in <lambda>\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471272126906_0001/container_1471272126906_0001_01_000022/pyspark.zip/pyspark/sql/types.py\", line 1484, in __getattr__\n    raise AttributeError(item)\nAttributeError: split\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:910)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:910)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:668)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:281)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-27f53bc231fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/mllib/feature.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset should be an RDD of term frequency vectors\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mjmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fitIDF\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminDocFreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_to_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mIDFModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[0;34m(name, *args)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[0;34m(sc, func, *args)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[0;32m--> 933\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    311\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o401.fitIDF.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 75.0 failed 4 times, most recent failure: Lost task 0.3 in stage 75.0 (TID 4411, ip-172-31-36-27.ec2.internal): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471272126906_0001/container_1471272126906_0001_01_000022/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471272126906_0001/container_1471272126906_0001_01_000022/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471272126906_0001/container_1471272126906_0001_01_000022/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-90-8a89e8eb336d>\", line 2, in <lambda>\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471272126906_0001/container_1471272126906_0001_01_000022/pyspark.zip/pyspark/sql/types.py\", line 1484, in __getattr__\n    raise AttributeError(item)\nAttributeError: split\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:910)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:910)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:668)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:281)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1934)\n\tat org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:983)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:965)\n\tat org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1108)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1085)\n\tat org.apache.spark.mllib.feature.IDF.fit(IDF.scala:54)\n\tat org.apache.spark.mllib.feature.IDF.fit(IDF.scala:67)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.fitIDF(PythonMLLibAPI.scala:658)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471272126906_0001/container_1471272126906_0001_01_000022/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471272126906_0001/container_1471272126906_0001_01_000022/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471272126906_0001/container_1471272126906_0001_01_000022/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-90-8a89e8eb336d>\", line 2, in <lambda>\n  File \"/mnt/yarn/usercache/hadoop/appcache/application_1471272126906_0001/container_1471272126906_0001_01_000022/pyspark.zip/pyspark/sql/types.py\", line 1484, in __getattr__\n    raise AttributeError(item)\nAttributeError: split\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:332)\n\tat org.apache.spark.rdd.RDD$$anonfun$8.apply(RDD.scala:330)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:935)\n\tat org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:910)\n\tat org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:910)\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:668)\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:281)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "idf = IDF().fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-b741081b8998>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mcnt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwc_per_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             raise AttributeError(\n\u001b[0;32m--> 844\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m    845\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def wc_per_row(row):\n",
    "    cnt = Counter()\n",
    "    for word in row:\n",
    "        cnt[word] += 1\n",
    "    return cnt.items() \n",
    "tf = corpus.map(lambda (x, y): (x, wc_per_row(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def wc_per_row(row):\n",
    "    cnt = Counter()\n",
    "    for word in row:\n",
    "        cnt[word] += 1\n",
    "    return cnt.items() \n",
    "\n",
    "tf = corpus.map(lambda (x, y): (x, wc_per_row(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = inv_index.map(lambda (x, y): (x, len(y)))\n",
    "num_documnents = tf.count()\n",
    "\n",
    "# At this step you can also apply some filters to make sure to keep\n",
    "# only terms within a 'good' range of df. \n",
    "import math.log10\n",
    "idf = df.map(lambda (k, v): (k, 1. + log10(num_documents/v))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Be Implemented...\n",
    "- Data cleansing\n",
    "- Histograms and distributions of content\n",
    "- NLP on free text (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
